Questions:
Where does the work take place?
    Hybrid method
        estep
            proposes a stick weights and pcfg weights
            iterates over strings, samples parse tree for each string, updates sufficient statistics
        mstep 
            accumulates sufficient stats
What is the input?
    Whole program:
        grammar file, input document (strings on each line), hyperparameters 
    Driving code:
        same as whole program, with training iterations (this in launch_train.py)
    Per iteration:
        learning:
            input_strings, number of processes
What is the output?
    Training:
        adagram- files that have subtrees parses at that iteration (show)
    Testing:
        document with input strings, document with output strings (show)
How is data represented?
    How are terminals 
        whole grammar represented as NLTK grammar object, so terminal can be any hashable object in python
    How are subtrees represented?
        inherits from nltk.grammar.Production, with some added
How could you combine it with other code?
    do training and export iterations manually
    would be easy since it automatically does them in batches (so collect ~10 and run it, for example)


Commands:

python2 -m launch_train --input_directory=demo/to_run --output_directory=demo --grammar_file=demo/to_run/unigram.grammar --number_of_documents=1000 --batch_size=10

 python2 -m launch_test --input_directory=demo/to_run --model_directory=demo/to_run/17Jun20-150139-D1000-P10-S10-B10-O100-t64-k0.75-Gunigram.grammar --non_terminal_symbol=Word